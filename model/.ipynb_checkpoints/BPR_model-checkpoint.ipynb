{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu,lib.cnmem=0.7,floatX=float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "import theano, numpy\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "train = pickle.load(open(data_path + 'train_feature_myadding1_only', 'rb'))\n",
    "print 'loading done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "test = pickle.load( open(data_path + 'test_feature_myadding1_only', 'rb'))\n",
    "print 'loading done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done\n",
      "    msno  song_id  target\n",
      "0  20479   135901       1\n",
      "1   4676   257686       1\n",
      "2   4676   273457       1\n",
      "3   4676    49669       1\n",
      "4  20479    39092       1\n",
      "34403 584719\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "user_ID_map = pickle.load( open(data_path + 'user_ID_map', 'rb'))\n",
    "songs_ID_map = pickle.load( open(data_path + 'songs_ID_map', 'rb'))\n",
    "print 'loading done'\n",
    "print train.head()\n",
    "num_users = len(user_ID_map)\n",
    "num_items = len(songs_ID_map)\n",
    "print num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BPR_Item(object):\n",
    "\n",
    "    def __init__(self, rank, n_users, n_items, lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self._learning_rate = learning_rate\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        \"\"\"\n",
    "          Configures Theano to run in fast mode\n",
    "          and using 32-bit floats. \n",
    "        \"\"\"\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "\n",
    "    def _generate_train_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lvector('i')\n",
    "        j = T.lvector('j')\n",
    "\n",
    "        h = numpy.random.random((self._n_items, self._rank))\n",
    "        b = numpy.random.random(self._n_items)\n",
    "        \n",
    "        self.W = theano.shared(numpy.random.random((self._n_users, self._rank)).astype('float32'), name='W')\n",
    "        self.H = theano.shared(h.astype('float32'), name='H')\n",
    "        self.B = theano.shared(b.astype('float32'), name='B')\n",
    "        \n",
    "        x_ui = T.dot(self.W[u], self.H[i].T).diagonal() + self.B[i]\n",
    "        x_uj = T.dot(self.W[u], self.H[j].T).diagonal() + self.B[j]\n",
    "        x_uij = T.nnet.sigmoid(x_ui-x_uj)\n",
    "        \n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.W[u] ** 2).sum(axis=1) - \n",
    "                    self._lambda_i * (self.H[i] ** 2).sum(axis=1) - self._lambda_j * \n",
    "                    (self.H[j] ** 2).sum(axis=1) - self._lambda_bias * \n",
    "                    (self.B[i] ** 2 + self.B[j] ** 2) )\n",
    "       \n",
    "    \n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_W = T.grad(cost=cost, wrt=self.W)\n",
    "        g_cost_H = T.grad(cost=cost, wrt=self.H)\n",
    "        g_cost_B = T.grad(cost=cost, wrt=self.B)\n",
    "\n",
    "        updates = [(self.W, self.W - self._learning_rate * g_cost_W), (self.H, self.H - self._learning_rate * g_cost_H), \n",
    "                   (self.B, self.B - self._learning_rate * g_cost_B) ]\n",
    "\n",
    "        self.train_model = theano.function(inputs=[u, i, j], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, train1=train, batch_size=1000):\n",
    "        \"\"\"\n",
    "          Trains the BPR Matrix Factorisation model using Stochastic\n",
    "          Gradient Descent and minibatches over `train_data`.\n",
    "          `train_data` is an array of (user_index, item_index) tuples.\n",
    "          We first create a set of random samples from `train_data` for \n",
    "          training, of size `epochs` * size of `train_data`.\n",
    "          We then iterate through the resulting training samples by\n",
    "          batches of length `batch_size`, and run one iteration of gradient\n",
    "          descent for the batch.\n",
    "        \"\"\"\n",
    "        if len(train1) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train)))\n",
    "            batch_size = len(train1)\n",
    "        \n",
    "        z = 0\n",
    "        n_sgd_samples = len(train1)\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        k = 5\n",
    "        for k in range(k):\n",
    "            while (z+1)*batch_size < len(train1):\n",
    "                self.train_model(\n",
    "                    train1[z*batch_size: (z+1)*batch_size].msno.tolist(),\n",
    "                    train1[z*batch_size: (z+1)*batch_size].song_id_x.tolist(),\n",
    "                    train1[z*batch_size: (z+1)*batch_size].song_id_y.tolist()\n",
    "                )\n",
    "                z += 1\n",
    "                t2 = time.time()\n",
    "                sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "                sys.stderr.flush()\n",
    "                t1 = t2\n",
    "        if len(train1) > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/len(train1)))\n",
    "            sys.stderr.flush()\n",
    "            \n",
    "    def _generate_test_model_function(self):\n",
    "        \"\"\"\n",
    "          Computes item predictions for `user_index`.\n",
    "          Returns an array of prediction values for each item\n",
    "          in the dataset.\n",
    "        \"\"\"\n",
    "        u = T.lvector('u')\n",
    "        i = T.lvector('i')\n",
    "        j = T.lvector('j')\n",
    "\n",
    "        x_ui = T.dot(self.W[u], self.H[i].T).diagonal() + self.B[i]\n",
    "        x_uj = T.dot(self.W[u], self.H[j].T).diagonal() + self.B[j]\n",
    "        x_uij = x_ui-x_uj\n",
    "        \n",
    "        self.test_model = theano.function(inputs=[u, i, j], outputs=x_uij)\n",
    "   \n",
    "    def test_bundle(self, train = train, batch_size=1000):\n",
    "        \"\"\"\n",
    "          Computes the Area Under Curve (AUC) on `test_data`.\n",
    "          `test_data` is an array of (user_index, item_index) tuples.\n",
    "          During this computation we ignore users and items\n",
    "          that didn't appear in the training data, to allow\n",
    "          for non-overlapping training and testing sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(train)\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list=self.test_model(\n",
    "                train[z*batch_size: (z+1)*batch_size].msno.tolist(),\n",
    "                train[z*batch_size: (z+1)*batch_size].song_id_x.tolist(),\n",
    "                train[z*batch_size: (z+1)*batch_size].song_id_y.tolist()\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            auc = np.sum([1.0 if a>0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        \n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return numpy.mean(auc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cols = ['msno', 'song_id', 'target']\n",
    "# train = train[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test = test[['msno', 'song_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(test,  open(data_path + 'test_feature_myadding1_only', 'wb'))\n",
    "# print 'OK'\n",
    "# pickle.dump(train,  open(data_path + 'train_feature_myadding1_only', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = int(len(train)*0.8)\n",
    "length = int(len(train)*0.2)\n",
    "trainSet = train[length:]\n",
    "#trainSet = train[:length]\n",
    "#valSet = train[length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print len(trainSet), len(valSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          msno  song_id\n",
      "1475483  17258   227855\n",
      "1475484  17258    76790\n",
      "1475485  21634   257890\n",
      "1475486  16505   355344\n",
      "1475487  16505    18739\n",
      "    msno  song_id_x  song_id_y\n",
      "0  17258     227855     240153\n",
      "1  17258     227855     118860\n",
      "2  17258     227855     253196\n",
      "3  17258     227855     221229\n",
      "4  17258     227855      55706\n",
      "msno         int64\n",
      "song_id_x    int64\n",
      "song_id_y    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_pos = trainSet[trainSet.target == 1][['msno', 'song_id']]\n",
    "train_neg = trainSet[trainSet.target == 0][['msno', 'song_id']]\n",
    "train_pos[\"msno\"] = train_pos[\"msno\"].astype('int')\n",
    "train_neg[\"msno\"] = train_neg[\"msno\"].astype('int')\n",
    "print train_pos.head()\n",
    "train = train_pos.merge(train_neg[['msno', 'song_id']]  , on='msno', how='inner')\n",
    "print train.head()\n",
    "print train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val_pos = valSet[valSet.target == 1][['msno', 'song_id']]\n",
    "# val_neg = valSet[valSet.target == 0][['msno', 'song_id']]\n",
    "# val_pos[\"msno\"] = val_pos[\"msno\"].astype('int')\n",
    "# val_neg[\"msno\"] = val_neg[\"msno\"].astype('int')\n",
    "# print val_pos.head()\n",
    "# val = val_pos.merge(val_neg[['msno', 'song_id']]  , on='msno', how='inner')\n",
    "# print val.head()\n",
    "# print val.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712702773\n"
     ]
    }
   ],
   "source": [
    "print len(train)\n",
    "sample = np.random.choice(range(len(train)), len(train)//10, replace = False)\n",
    "#train1 = pd.DataFrame(np.random.choice(range(len(train)), len(train) // 100, replace = False), columns=train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = train.iloc[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print len(train1)\n",
    "# print len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpr_item = BPR_Item(50, num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 71270000 ( 100.00% ) in 0.1104 seconds\n",
      "Total training time 8462.72 seconds; 1.187412e-04 per sample\n"
     ]
    }
   ],
   "source": [
    "# bpr_item.train(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (71270000 samples): 0.95294onds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95294315981478872"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bpr_item.test_bundle(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_path = '../data/'\n",
    "# pickle.dump(bpr_item, open(data_path + 'bpr_item', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H_item = np.matrix(bpr_item.H.eval())\n",
    "B_item = np.matrix(bpr_item.B.eval()).T\n",
    "W_item = np.matrix(bpr_item.W.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(584719, 1) (34403, 50) (584719, 50)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(B_item), np.shape(W_item), np.shape(H_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos['user_weight'] = train_pos.msno.apply(lambda x : W_item[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos['item_weight'] = train_pos.song_id.apply(lambda x : H_item[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos['item_bias'] = train_pos.song_id.apply(lambda x : B_item[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos['score'] = train_pos.apply(lambda x: (B_item[x['song_id']] + H_item[x['song_id']]\\\n",
    "                                                * W_item[x['msno']].T)[0,0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_neg['score'] = train_neg.apply(lambda x: (B_item[x['song_id']] + H_item[x['song_id']]\\\n",
    "                                                * W_item[x['msno']].T)[0,0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos1 = train_pos.drop(['user_weight', 'item_weight', 'item_bias'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno         int64\n",
      "song_id      int64\n",
      "score      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print train_pos1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "pickle.dump(train_pos1, open(data_path + 'train_pos_bpr', 'wb'))\n",
    "print 'done'\n",
    "pickle.dump(train_neg, open(data_path + 'train_neg_bpr', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in train_neg.msno.unique():\n",
    "    if user in user_limit: continue\n",
    "    score = np.max(train_neg[train_neg.msno == user]['score'])\n",
    "    user_limit[user] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_limit = {}\n",
    "for user in train_pos.msno.unique():\n",
    "    pos = np.min(train_pos[train_pos.msno == user]['score'])\n",
    "    user_limit[user] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['score'] = test.apply(lambda x: (B_item[x['song_id']] + H_item[x['song_id']]\\\n",
    "                                                * W_item[x['msno']].T)[0,0], axis = 1)\n",
    "# test.msno.apply(lambda x: user_limit[x] if x in user_limit else 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno         int64\n",
      "song_id      int64\n",
      "target     float64\n",
      "score      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[\"msno\"] = test[\"msno\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target(row):\n",
    "    if row['msno'] not in user_limit:\n",
    "        return 0.5\n",
    "    return 1 if row['score'] > user_limit[row['msno']] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['target'] = test.apply(lambda x: get_target(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['target']  = test['target'].apply(lambda x: 1 if True else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "#test = pickle.load( open(data_path + 'test_feature_myadding', 'rb'))\n",
    "test1 = pickle.load( open(data_path + 'test_feature_myadding1_reduceDim', 'rb'))\n",
    "print 'loading done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print test['target'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions Model model of gbdt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print ('Saving predictions Model model of gbdt')\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = test1['id'].values\n",
    "subm['target'] = test['target'].values\n",
    "subm.to_csv(data_path + '../submissions/submission_bpr.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bpr_item.test_bundle(val)\n",
    "#2196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#20\n",
    "#0.81675onds\n",
    "#0.54\n",
    "#//100\n",
    "#50\n",
    "#0.88\n",
    "#0.55\n",
    "\n",
    "#//50\n",
    "#50\n",
    "#0.91\n",
    "#0.55\n",
    "\n",
    "#//10\n",
    "#50\n",
    "#0.95\n",
    "#0.546\n",
    "\n",
    "#//all train //10\n",
    "#0.95294onds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
